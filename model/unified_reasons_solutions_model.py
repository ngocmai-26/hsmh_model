#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Reasons & Solutions Model
M√¥ h√¨nh th·ªëng nh·∫•t cho 6 lo·∫°i reasons & solutions:
1. Teaching Methods (PPGD)
2. Evaluation Methods (PPDG)
3. Student Conduct (ƒêi·ªÉm r√®n luy·ªán)
4. Academic Midterm (ƒêi·ªÉm gi·ªØa k·ª≥)
5. CLO Attendance (Chuy√™n c·∫ßn CLO)
6. Self-Study (T·ª± h·ªçc)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

class UnifiedReasonsSolutionsModel:
    """M√¥ h√¨nh th·ªëng nh·∫•t cho t·∫•t c·∫£ c√°c lo·∫°i reasons & solutions"""
    
    # ƒê·ªãnh nghƒ©a c√°c file d·ªØ li·ªáu
    DATASET_FILES = {
        'teaching_methods': 'dulieu/teaching_methods_reason_solution_dataset_v2_5000.csv',
        'evaluation_methods': 'dulieu/evaluation_methods_reason_solution_dataset_v2_5000.csv',
        'student_conduct': 'dulieu/student_conduct_reason_solution_dataset_v1_5000.csv',
        'academic_midterm': 'dulieu/academic_midterm_reason_solution_dataset_5000.csv',
        'clo_attendance': 'dulieu/clo_attendance_reason_solution_dataset_5000.csv',
        'self_study': 'dulieu/student_selfstudy_reason_solution_dataset_v1_5000.csv'
    }
    
    # M√¥ t·∫£ t·ª´ng dataset
    DATASET_DESCRIPTIONS = {
        'teaching_methods': 'Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y (PPGD)',
        'evaluation_methods': 'Ph∆∞∆°ng ph√°p ƒë√°nh gi√° (PPDG)',
        'student_conduct': 'ƒêi·ªÉm r√®n luy·ªán',
        'academic_midterm': 'ƒêi·ªÉm gi·ªØa k·ª≥',
        'clo_attendance': 'Chuy√™n c·∫ßn CLO',
        'self_study': 'T·ª± h·ªçc'
    }
    
    def __init__(self):
        """Kh·ªüi t·∫°o model"""
        self.datasets = {}
        self.models = {}
        self.label_encoders = {}
        self.severity_encoders = {}
        
    def load_all_datasets(self):
        """T·∫£i t·∫•t c·∫£ c√°c datasets"""
        print("=" * 80)
        print("ƒêANG T·∫¢I T·∫§T C·∫¢ C√ÅC DATASETS")
        print("=" * 80)
        
        for key, filepath in self.DATASET_FILES.items():
            try:
                df = pd.read_csv(filepath, encoding='utf-8')
                self.datasets[key] = df
                print(f"‚úÖ {self.DATASET_DESCRIPTIONS[key]:30} | {len(df):5} b·∫£n ghi | {filepath}")
            except Exception as e:
                print(f"‚ùå {self.DATASET_DESCRIPTIONS[key]:30} | L·ªói: {e}")
                
        print(f"\nüìä T·ªïng s·ªë datasets: {len(self.datasets)}/{len(self.DATASET_FILES)}")
        return len(self.datasets) > 0
    
    def analyze_dataset_structure(self):
        """Ph√¢n t√≠ch c·∫•u tr√∫c c·ªßa t·ª´ng dataset"""
        print("\n" + "=" * 80)
        print("PH√ÇN T√çCH C·∫§U TR√öC C√ÅC DATASETS")
        print("=" * 80)
        
        for key, df in self.datasets.items():
            print(f"\nüìã {self.DATASET_DESCRIPTIONS[key]}")
            print(f"   S·ªë c·ªôt: {len(df.columns)}")
            print(f"   C√°c c·ªôt: {', '.join(df.columns.tolist())}")
            
            # Ph√¢n t√≠ch severity_level
            if 'severity_level' in df.columns:
                severity_dist = df['severity_level'].value_counts()
                print(f"   Ph√¢n b·ªë Severity:")
                for level, count in severity_dist.items():
                    print(f"      - {level}: {count} ({count/len(df)*100:.1f}%)")
    
    def prepare_training_data(self, dataset_key):
        """Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán cho m·ªôt dataset c·ª• th·ªÉ"""
        if dataset_key not in self.datasets:
            print(f"‚ùå Dataset {dataset_key} kh√¥ng t·ªìn t·∫°i!")
            return None, None, None, None
        
        df = self.datasets[dataset_key].copy()
        
        # X√°c ƒë·ªãnh c·ªôt features
        feature_cols = []
        target_col = 'severity_level'
        
        # T√πy ch·ªânh features theo t·ª´ng dataset
        if dataset_key == 'teaching_methods':
            if 'teaching_method_pred' in df.columns:
                feature_cols.append('teaching_method_pred')
        elif dataset_key == 'evaluation_methods':
            if 'evaluation_method_pred' in df.columns:
                feature_cols.append('evaluation_method_pred')
        elif dataset_key == 'student_conduct':
            if 'conduct_score_pred' in df.columns:
                feature_cols.append('conduct_score_pred')
        elif dataset_key == 'academic_midterm':
            if 'midterm_score' in df.columns:
                feature_cols.append('midterm_score')
        elif dataset_key == 'clo_attendance':
            if 'clo_score_pred' in df.columns:
                feature_cols.append('clo_score_pred')
        elif dataset_key == 'self_study':
            # Th√™m features cho self_study n·∫øu c√≥
            pass
        
        # Th√™m text features
        if 'reason_text' in df.columns:
            df['reason_length'] = df['reason_text'].str.len()
            feature_cols.append('reason_length')
            
        if 'solution_text' in df.columns:
            df['solution_length'] = df['solution_text'].str.len()
            feature_cols.append('solution_length')
        
        # Encode target
        if target_col not in df.columns:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt {target_col}")
            return None, None, None, None
            
        le = LabelEncoder()
        y = le.fit_transform(df[target_col])
        self.severity_encoders[dataset_key] = le
        
        # T·∫°o X
        if not feature_cols:
            print(f"‚ùå Kh√¥ng c√≥ features cho dataset {dataset_key}")
            return None, None, None, None
            
        X = df[feature_cols].fillna(0)
        
        return X, y, df, feature_cols
    
    def train_model(self, dataset_key, test_size=0.2, random_state=42):
        """Hu·∫•n luy·ªán m√¥ h√¨nh cho m·ªôt dataset c·ª• th·ªÉ"""
        print(f"\n{'=' * 80}")
        print(f"HU·∫§N LUY·ªÜN M√î H√åNH: {self.DATASET_DESCRIPTIONS[dataset_key]}")
        print(f"{'=' * 80}")
        
        X, y, df, feature_cols = self.prepare_training_data(dataset_key)
        
        if X is None:
            print(f"‚ùå Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh cho {dataset_key}")
            return None
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        print(f"üìä D·ªØ li·ªáu:")
        print(f"   - Train: {len(X_train)} m·∫´u")
        print(f"   - Test:  {len(X_test)} m·∫´u")
        print(f"   - Features: {feature_cols}")
        
        # Train Random Forest
        rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=2,
            random_state=random_state,
            class_weight='balanced'
        )
        
        rf_model.fit(X_train, y_train)
        rf_score = rf_model.score(X_test, y_test)
        
        print(f"\n‚úÖ Random Forest Accuracy: {rf_score:.4f}")
        
        # Train Gradient Boosting
        gb_model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=10,
            learning_rate=0.1,
            random_state=random_state
        )
        
        gb_model.fit(X_train, y_train)
        gb_score = gb_model.score(X_test, y_test)
        
        print(f"‚úÖ Gradient Boosting Accuracy: {gb_score:.4f}")
        
        # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
        if rf_score >= gb_score:
            self.models[dataset_key] = {
                'model': rf_model,
                'type': 'RandomForest',
                'accuracy': rf_score,
                'features': feature_cols,
                'data': df
            }
            print(f"\nüèÜ Ch·ªçn Random Forest (accuracy: {rf_score:.4f})")
        else:
            self.models[dataset_key] = {
                'model': gb_model,
                'type': 'GradientBoosting',
                'accuracy': gb_score,
                'features': feature_cols,
                'data': df
            }
            print(f"\nüèÜ Ch·ªçn Gradient Boosting (accuracy: {gb_score:.4f})")
        
        return self.models[dataset_key]
    
    def train_all_models(self):
        """Hu·∫•n luy·ªán t·∫•t c·∫£ c√°c m√¥ h√¨nh"""
        print("\n" + "=" * 80)
        print("B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN T·∫§T C·∫¢ C√ÅC M√î H√åNH")
        print("=" * 80)
        
        results = {}
        for key in self.datasets.keys():
            result = self.train_model(key)
            if result:
                results[key] = result
        
        print("\n" + "=" * 80)
        print("K·∫æT QU·∫¢ HU·∫§N LUY·ªÜN")
        print("=" * 80)
        
        for key, result in results.items():
            print(f"{self.DATASET_DESCRIPTIONS[key]:30} | "
                  f"{result['type']:20} | "
                  f"Accuracy: {result['accuracy']:.4f}")
        
        return len(results)
    
    def predict_reason_solution(self, dataset_key, features, top_k=3):
        """D·ª± ƒëo√°n reasons & solutions cho m·ªôt dataset c·ª• th·ªÉ"""
        if dataset_key not in self.models:
            return {
                'error': f'Model cho {dataset_key} ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán'
            }
        
        model_info = self.models[dataset_key]
        model = model_info['model']
        df = model_info['data']
        feature_names = model_info['features']
        
        # T·∫°o features ƒë·∫ßy ƒë·ªß
        # features ch·ªâ ch·ª©a score, c·∫ßn th√™m reason_length v√† solution_length
        full_features = list(features)
        
        # Th√™m gi√° tr·ªã m·∫∑c ƒë·ªãnh cho reason_length v√† solution_length n·∫øu c·∫ßn
        while len(full_features) < len(feature_names):
            full_features.append(100)  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh cho length
        
        # D·ª± ƒëo√°n severity
        X = np.array([full_features]).reshape(1, -1)
        severity_pred = model.predict(X)[0]
        severity_proba = model.predict_proba(X)[0]
        
        # Decode severity
        severity_label = self.severity_encoders[dataset_key].inverse_transform([severity_pred])[0]
        severity_confidence = severity_proba[severity_pred]
        
        # L·∫•y top_k reasons & solutions
        filtered_df = df[df['severity_level'] == severity_label].copy()
        
        if len(filtered_df) == 0:
            filtered_df = df.copy()
        
        # Random sample top_k
        if len(filtered_df) > top_k:
            samples = filtered_df.sample(n=top_k, random_state=42)
        else:
            samples = filtered_df.sample(n=min(len(filtered_df), top_k), random_state=42)
        
        results = []
        for idx, row in samples.iterrows():
            results.append({
                'reason': row['reason_text'],
                'solution': row['solution_text'],
                'severity': severity_label,
                'confidence': float(severity_confidence)
            })
        
        return {
            'dataset': self.DATASET_DESCRIPTIONS[dataset_key],
            'severity_level': severity_label,
            'severity_confidence': float(severity_confidence),
            'results': results
        }
    
    def get_model_summary(self):
        """L·∫•y t√≥m t·∫Øt v·ªÅ c√°c models"""
        summary = {
            'total_datasets': len(self.datasets),
            'total_models': len(self.models),
            'models': {}
        }
        
        for key, model_info in self.models.items():
            summary['models'][key] = {
                'description': self.DATASET_DESCRIPTIONS[key],
                'type': model_info['type'],
                'accuracy': model_info['accuracy'],
                'features': model_info['features']
            }
        
        return summary


def main():
    """Demo s·ª≠ d·ª•ng model"""
    print("=" * 80)
    print("UNIFIED REASONS & SOLUTIONS MODEL - DEMO")
    print("=" * 80)
    
    # Kh·ªüi t·∫°o model
    model = UnifiedReasonsSolutionsModel()
    
    # Load datasets
    if not model.load_all_datasets():
        print("‚ùå Kh√¥ng th·ªÉ t·∫£i datasets!")
        return
    
    # Ph√¢n t√≠ch c·∫•u tr√∫c
    model.analyze_dataset_structure()
    
    # Hu·∫•n luy·ªán t·∫•t c·∫£ models
    num_trained = model.train_all_models()
    print(f"\n‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng {num_trained} models!")
    
    # Demo prediction
    print("\n" + "=" * 80)
    print("DEMO D·ª∞ ƒêO√ÅN")
    print("=" * 80)
    
    # V√≠ d·ª•: D·ª± ƒëo√°n teaching methods
    if 'teaching_methods' in model.models:
        result = model.predict_reason_solution('teaching_methods', [0.5], top_k=3)
        print(f"\nüìö {result['dataset']}")
        print(f"   Severity: {result['severity_level']} (confidence: {result['severity_confidence']:.3f})")
        for i, item in enumerate(result['results'], 1):
            print(f"\n   {i}. Nguy√™n nh√¢n: {item['reason'][:100]}...")
            print(f"      Gi·∫£i ph√°p: {item['solution'][:100]}...")
    
    # Hi·ªÉn th·ªã summary
    print("\n" + "=" * 80)
    print("T√ìM T·∫ÆT M√î H√åNH")
    print("=" * 80)
    summary = model.get_model_summary()
    print(f"T·ªïng s·ªë datasets: {summary['total_datasets']}")
    print(f"T·ªïng s·ªë models: {summary['total_models']}")


if __name__ == "__main__":
    main()

